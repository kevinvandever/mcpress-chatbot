# Story-005: Document Processing Pipeline

## Overview

Unified document processing service replacing fragmented async_upload.py with:
- **Persistent job tracking** (survives restarts)
- **Error recovery** with exponential backoff (5s, 15s, 60s)
- **Webhook notifications** for external integrations
- **Storage optimization** (deduplication, metrics)
- **Comprehensive API** for job management

---

## Architecture

### Processing Pipeline Stages

```
1. QUEUED     â†’ Job created, waiting to start
2. EXTRACTING â†’ PDF text/image/code extraction
3. CHUNKING   â†’ Text splitting and deduplication
4. EMBEDDING  â†’ Vector generation (if applicable)
5. STORING    â†’ Database persistence
6. COMPLETED  â†’ All stages finished successfully
7. FAILED     â†’ Error occurred, check error_message
```

### Components

| Component | Purpose | File |
|-----------|---------|------|
| **ProcessingJob** | Job model with Pydantic validation | `processing_models.py` |
| **ProcessingEvent** | Audit trail events | `processing_models.py` |
| **DocumentProcessingService** | Main processing orchestrator | `document_processing_service.py` |
| **ErrorRecovery** | Retry logic with backoff | `document_processing_service.py` |
| **StorageOptimizer** | Deduplication and metrics | `document_processing_service.py` |
| **API Routes** | FastAPI endpoints | `processing_routes.py` |
| **Integration** | Wire into main.py | `processing_integration.py` |

---

## Deployment

### Prerequisites

```bash
# Python dependencies (already in requirements.txt)
- asyncpg
- pydantic
- aiohttp (for webhooks)
```

### Step 1: Run Database Migration

```bash
cd backend
python run_migration.py migrate
```

Expected output:
```
ðŸ”„ Running migration: 001_processing_pipeline.sql
âœ… Migration completed successfully!

ðŸ“‹ Created tables:
   - processing_jobs: 0 rows
   - processing_events: 0 rows
   - storage_metrics: 0 rows
```

### Step 2: Verify Integration

The service is automatically initialized on Railway startup (see `main.py:367-372`).

Check logs for:
```
ðŸ“¦ Story-005: Processing pipeline module loaded
âœ… Document Processing Service ready (Story-005)
âœ… Processing pipeline endpoints enabled at /api/process/*
```

### Step 3: Test Health Endpoint

```bash
curl https://your-railway-app.railway.app/api/process/health
```

Expected response:
```json
{
  "status": "healthy",
  "database": "connected",
  "last_24h_stats": {
    "queued": 0,
    "processing": 0,
    "completed": 0,
    "failed": 0
  }
}
```

---

## API Endpoints

### Start Processing

```bash
POST /api/process/document
Content-Type: application/json

{
  "file_path": "/app/uploads/test.pdf",
  "filename": "test.pdf",
  "metadata": {
    "category": "technical",
    "title": "Test Document"
  },
  "webhook_url": "https://example.com/webhook" # optional
}
```

Response:
```json
{
  "job_id": "job_a1b2c3...",
  "filename": "test.pdf",
  "stage": "queued",
  "progress": 0,
  "created_at": "2025-10-13T12:00:00"
}
```

### Get Job Status

```bash
GET /api/process/job/{job_id}
```

Response:
```json
{
  "job": {
    "job_id": "job_a1b2c3...",
    "filename": "test.pdf",
    "stage": "storing",
    "progress": 70,
    "retry_count": 0,
    "error_message": null
  },
  "recent_events": [
    {
      "event_type": "STORING_COMPLETE",
      "stage": "storing",
      "message": "Stored in vector database",
      "created_at": "2025-10-13T12:01:30"
    }
  ]
}
```

### List All Jobs

```bash
GET /api/process/jobs?page=1&page_size=50&stage=completed
```

### Retry Failed Job

```bash
POST /api/process/retry/{job_id}
```

### Cleanup Old Jobs

```bash
POST /api/process/cleanup?days_old=30
```

### Get Storage Metrics

```bash
GET /api/process/metrics
```

Response:
```json
{
  "total_documents": 115,
  "total_embeddings": 12450,
  "storage_bytes": 52428800,
  "storage_mb": 50.0,
  "avg_chunks_per_doc": 108.26
}
```

---

## Webhook Events

If `webhook_url` is provided, the service sends notifications:

### Event Types

| Event | When | Payload Includes |
|-------|------|------------------|
| `processing.started` | Job begins | job_id, filename, metadata |
| `processing.progress` | Stage changes | stage, progress (0-100) |
| `processing.completed` | Success | final metadata, stats |
| `processing.failed` | Error | error_message, retry_count |

### Webhook Payload Example

```json
{
  "event": "processing.completed",
  "job_id": "job_a1b2c3...",
  "filename": "test.pdf",
  "stage": "completed",
  "progress": 100,
  "metadata": {
    "total_pages": 250,
    "has_images": true,
    "has_code": false
  },
  "timestamp": "2025-10-13T12:02:00"
}
```

---

## Error Recovery

### Automatic Retries

- **Max Attempts**: 3
- **Backoff Delays**: 5s, 15s, 60s (exponential)
- **Retry Stages**: All (extraction, chunking, storing)

### Example Retry Scenario

```
Attempt 1: Extract PDF â†’ FAIL (timeout)
Wait 5 seconds...
Attempt 2: Extract PDF â†’ FAIL (connection)
Wait 15 seconds...
Attempt 3: Extract PDF â†’ SUCCESS
```

Job updates:
```json
{
  "retry_count": 2,
  "stage": "extracting"
}
```

### Manual Retry

If job fails after 3 attempts:
```bash
POST /api/process/retry/{job_id}
```

---

## Storage Optimization

### Deduplication

- **Chunk-level**: Removes duplicate content within document
- **Saves**: ~20-30% storage on typical documents

### Metrics Tracking

Recorded to `storage_metrics` table:
- Total documents
- Total embeddings
- Storage bytes
- Average chunks per document

### Cleanup

Remove old completed/failed jobs:
```bash
POST /api/process/cleanup?days_old=30
```

Default: 30 days retention

---

## Monitoring

### Database Queries

Check job status directly:
```sql
-- Recent jobs
SELECT job_id, filename, stage, progress, created_at
FROM processing_jobs
ORDER BY created_at DESC
LIMIT 20;

-- Failed jobs
SELECT job_id, filename, error_message, retry_count
FROM processing_jobs
WHERE stage = 'failed'
ORDER BY created_at DESC;

-- Processing time analysis
SELECT
  filename,
  EXTRACT(EPOCH FROM (completed_at - created_at)) as seconds
FROM processing_jobs
WHERE stage = 'completed'
ORDER BY seconds DESC;
```

### Event Audit Trail

```sql
-- Recent events for a job
SELECT event_type, stage, message, created_at
FROM processing_events
WHERE job_id = 'job_a1b2c3...'
ORDER BY created_at ASC;
```

---

## Rollback

### Option 1: Feature Flag

Disable new pipeline without code changes:
```bash
# Railway environment variable
USE_NEW_PROCESSING_PIPELINE=false
```

Old `async_upload.py` continues to work.

### Option 2: Database Rollback

```bash
cd backend
python run_migration.py rollback
```

Removes new tables, preserves `documents` table.

### Option 3: Git Revert

```bash
git checkout main
git branch -D feature/story-005-doc-processing-pipeline
```

---

## Testing

### Run Unit Tests

```bash
cd backend
pytest test_processing_pipeline.py -v
```

### Manual API Tests

```bash
# Test health
curl http://localhost:8000/api/process/health

# Test job creation (requires file)
curl -X POST http://localhost:8000/api/process/document \
  -H "Content-Type: application/json" \
  -d '{
    "file_path": "/tmp/test.pdf",
    "filename": "test.pdf",
    "metadata": {"category": "test"}
  }'

# Check job status
curl http://localhost:8000/api/process/job/{job_id}
```

---

## Troubleshooting

### Issue: "Processing service not initialized"

**Cause**: Database connection failed on startup

**Fix**:
1. Check `DATABASE_URL` environment variable
2. Verify database connectivity
3. Check Railway logs for init errors

### Issue: Jobs stuck in "queued" stage

**Cause**: Processing not starting

**Fix**:
1. Check if pool initialized: `service.pool is not None`
2. Verify file exists at `file_path`
3. Check for errors in `processing_events` table

### Issue: "Webhook delivery failed"

**Cause**: External endpoint unreachable

**Fix**:
- Webhooks are non-blocking - job continues even if webhook fails
- Check webhook endpoint logs
- Verify URL is reachable from Railway

---

## Performance

### Benchmarks (Railway Free Tier)

| Metric | Value |
|--------|-------|
| 100-page PDF | ~15-20 seconds |
| Concurrent jobs | Up to 3 |
| Database queries | < 100ms |
| Webhook delivery | < 2 seconds |

### Optimization Tips

1. **Batch Processing**: Use pagination for large job lists
2. **Cleanup**: Run cleanup weekly to remove old jobs
3. **Monitoring**: Check `storage_metrics` for growth trends

---

## Future Enhancements

Potential improvements for later stories:

- [ ] WebSocket support for real-time progress
- [ ] Priority queue (high/low priority jobs)
- [ ] Scheduled processing (cron-style)
- [ ] Resume partial failures (checkpoint recovery)
- [ ] Multi-file batch processing
- [ ] Processing rate limiting

---

## Support

**Documentation**: `STORY_005_INTEGRATION.md`
**Tests**: `test_processing_pipeline.py`
**Migration**: `backend/migrations/001_processing_pipeline.sql`
**Rollback**: `backend/migrations/001_processing_pipeline_rollback.sql`

**Story File**: `docs/prd/stories/story-005-doc-processing.md`
**Commit**: `66ae999` on `feature/story-005-doc-processing-pipeline`

---

**Status**: âœ… Ready for Deployment
**Last Updated**: 2025-10-13
**Version**: 1.0.0
